# OpenAI API 비교: Chat Completions vs. Assistants

이 문서는 현재 `evtool-app`의 코드 생성 기능에 사용되는 `Chat Completions API` 방식과 `Assistants API`를 사용하는 새로운 방식의 동작 원리, 장단점을 비교 분석하고, 향후 어떤 방식을 사용하는 것이 더 적합할지 추천하기 위해 작성되었습니다.

## 1. 현재 접근 방식: Chat Completions API

현재 시스템은 OpenAI의 `Chat Completions API` (`gpt-4-turbo` 모델)를 사용하고 있습니다.

### 동작 방식

1.  클라이언트가 코드 생성을 요청하면, 서버는 해당 보드의 JSON 데이터를 파일에서 읽습니다.
2.  연결된 다른 UML 다이어그램이 있다면 해당 JSON 데이터도 모두 읽어옵니다.
3.  읽어온 모든 모델 정보를 하나의 거대한 JSON 객체로 만듭니다.
4.  미리 정의된 시스템 프롬프트(역할, 지시사항)와 사용자 프롬프트(요구사항)에 위 JSON 데이터를 문자열 형태로 삽입하여 하나의 완성된 프롬프트를 생성합니다.
5.  이 프롬프트를 `Chat Completions API`에 전송하여 코드 생성을 요청하고, 결과를 스트리밍 받아 사용자에게 Zip 파일로 제공합니다.

이 방식은 상태가 없는(stateless) 단일 요청-응답 구조입니다.

### 장점

*   **단순성**: API 호출 한번으로 모든 작업이 완료되므로 로직이 직관적이고 구현이 간단합니다.
*   **완벽한 제어**: 개발자가 프롬프트의 모든 내용을 직접 구성하므로, 모델에 전달되는 내용을 완벽하게 제어할 수 있습니다.
*   **낮은 지연 시간**: 단일 작업의 경우, 여러 객체를 생성하고 상태를 관리해야 하는 Assistants API에 비해 잠재적으로 더 빠를 수 있습니다.

### 단점

*   **컨텍스트 창 한계**: **가장 치명적인 단점입니다.** 모델링이 복잡해져 JSON 데이터의 크기가 커지면, 모델이 한 번에 처리할 수 있는 입력 토큰 한계(Context Window)를 초과할 수 있습니다. 이 경우 API 요청이 실패하고 코드 생성이 불가능해집니다.
*   **비용 비효율성**: 사용자가 코드 생성을 요청할 때마다 매번 동일한 대용량 JSON 데이터를 프롬프트에 포함하여 전송해야 합니다. 이는 불필요한 입력 토큰 비용을 지속적으로 발생시킵니다.
*   **대화의 어려움**: "이전에 생성된 코드에 이러이러한 기능을 추가해줘" 와 같은 연속적인 대화형 리팩토링 기능을 구현하려면, 서버가 이전 대화 기록과 전체 모델 데이터를 모두 관리하고 매번 요청에 포함해야 하므로 매우 복잡해집니다.

## 2. 새로운 접근 방식: Assistants API

`Assistants API`는 특정 목적을 가진 AI 챗봇을 만들고, 대화의 상태를 OpenAI가 관리하며, 다양한 도구를 활용할 수 있도록 설계된 API입니다.

### 주요 개념

*   **Assistant**: 특정 지시사항, 모델, 도구를 가지고 있는 AI 에이전트입니다. (예: "DDD 전문가인 소프트웨어 아키텍트 Assistant")
*   **Thread**: 사용자와 Assistant 간의 대화 세션입니다. 대화 내용은 여기에 계속 기록됩니다.
*   **File Search (구 Retrieval)**: Assistant가 사용할 수 있는 도구 중 하나로, Assistant에게 파일을 제공하면 대화 중에 해당 파일의 내용을 참고하여 답변을 생성할 수 있게 해줍니다.
*   **Run**: Thread 내에서 Assistant를 실행하는 과정입니다. Assistant는 Thread의 대화 내용과 File Search 도구를 활용하여 적절한 답변을 생성합니다.

### 동작 방식 (구현 시나리오)

1.  **(최초 1회)**: "DDD 전문가 소프트웨어 아키텍트" 역할을 하는 **Assistant**를 미리 생성해 둡니다.
2.  **(코드 생성 요청 시)**:
    1.  서버는 사용자의 보드 데이터(JSON)를 임시 파일로 저장합니다.
    2.  이 JSON 파일을 OpenAI에 **업로드**하고, 생성된 파일 ID를 얻습니다.
    3.  사용자별로 새로운 **Thread**를 생성합니다.
    4.  업로드된 파일 ID를 Thread에 첨부하고, "이 파일을 참고해서 Java 코드를 생성해줘" 와 같은 **메시지**를 Thread에 추가합니다.
    5.  Assistant가 Thread를 처리하도록 **Run**을 시작시킵니다.
    6.  Assistant는 `File Search` 도구를 사용해 첨부된 JSON 파일의 내용을 분석하고, 지시사항에 따라 코드를 생성합니다.
    7.  서버는 Run의 상태를 주기적으로 확인(polling)하여 작업이 완료되면, Thread에서 Assistant가 생성한 최종 결과물을 가져와 사용자에게 제공합니다.

### 장점

*   **컨텍스트 한계 극복**: JSON 데이터를 프롬프트에 직접 넣는 대신 파일로 제공하므로, 모델의 컨텍스트 창 크기에 제약받지 않습니다. 아무리 복잡하고 큰 다이어그램이라도 처리할 수 있어 **확장성**이 뛰어납니다.
*   **상태 관리의 편리함**: API가 대화의 상태(Thread)를 자동으로 관리해줍니다. 이를 통해 "이전에 생성한 코드에서 이 부분만 바꿔줘" 와 같은 **대화형 기능 구현**이 매우 용이해집니다.
*   **비용 효율성**: 대용량 파일을 한번만 업로드하면 되므로, 대화가 길어지거나 여러 번 요청을 보내도 반복적인 토큰 비용이 발생하지 않아 장기적으로 더 경제적일 수 있습니다.

### 단점

*   **구현 복잡성**: Assistant, Thread, File, Run 등 여러 객체를 관리하고, 비동기적인 Run의 완료 상태를 계속 확인해야 하므로 초기 구현이 더 복잡합니다.
*   **추상화**: 모델이 파일을 어떻게 활용하여 답변을 생성하는지에 대한 제어권이 다소 줄어듭니다. Chat Completions 방식보다 내부 동작이 추상화되어 있습니다.
*   **잠재적 지연 시간**: 단일 요청의 경우, 여러 객체를 생성하고 비동기적으로 처리하는 과정에서 Chat Completions API보다 지연 시간이 더 길어질 수 있습니다.

## 3. 비교 요약 및 최종 추천

| 항목 | Chat Completions API (현재 방식) | Assistants API (추천 방식) |
| :--- | :--- | :--- |
| **핵심** | 단일 요청/응답, 상태 없음 | 대화 세션 기반, 상태 관리 |
| **컨텍스트 처리** | 프롬프트에 모두 포함 (크기 제한 O) | 파일 첨부 (크기 제한 X) |
| **대화형 기능** | 직접 구현 (복잡) | API 내장 (용이) |
| **구현 복잡도** | 낮음 | 높음 |
| **확장성** | 낮음 (모델 크기가 커지면 실패) | **높음** |
| **비용** | 대용량 데이터 반복 전송 시 비효율적 | 대용량 데이터 처리 및 대화 시 효율적 |

### 추천: Assistants API

장기적인 관점에서 **`Assistants API`로 전환하는 것을 강력하게 추천합니다.**

### 추천 이유

현재 방식의 가장 큰 문제점은 **확장성 부재**입니다. 사용자가 다이어그램을 정교하게 만들수록 JSON 데이터는 계속 커질 것이고, 이는 필연적으로 모델의 컨텍스트 창 한계를 초과하여 서비스의 핵심 기능인 코드 생성이 실패하는 상황으로 이어질 것입니다.

`Assistants API`와 `File Search` 도구는 바로 이 문제를 해결하기 위해 설계되었습니다. 초기 구현의 복잡성은 있지만, **서비스의 지속 가능성과 확장성**을 확보할 수 있으며, 향후 **대화형 코드 수정과 같은 고도화된 기능을 구현할 수 있는 기반**이 됩니다. 따라서 이는 단순한 기술 변경이 아닌, 앞으로의 서비스 발전을 위한 필수적인 아키텍처 변경이라 할 수 있습니다.
